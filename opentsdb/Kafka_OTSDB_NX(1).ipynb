{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#sys.path.insert(0, '/opt/cloudera/parcels/CDH/lib/spark/python/')\n",
    "#sys.path.insert(0, '/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.9-src.zip')\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-oracle/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/CDH/lib/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.9-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql.functions import explode\n",
    "from __future__ import print_function\n",
    "from pyspark.sql.types import *\n",
    "import requests\n",
    "import json\n",
    "import numbers\n",
    "import ast\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '8g')\n",
    "sc = SparkContext('local[4]', 'test-spark0')\n",
    "ssc = StreamingContext(sc, 1)\n",
    "kafkaParams = {\"metadata.broker.list\": \"pnda14.gspie.lab:9092,pnda15.gspie.lab:9092,pnda13.gspie.lab:9092\", \"auto.offset.reset\": \"largest\"}\n",
    "topic = \"telemetrynx1\"\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc,[topic],kafkaParams)\n",
    "\n",
    "#kafka_rdd = kafkaStream.map(lambda (k,v): v)\n",
    "kafka_rdd = kafkaStream.map(lambda v: json.loads(v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path1 = \"show interface counters\"\n",
    "path2 = \"show bgp all summary\"\n",
    "path3 = \"show hardware internal buffer info pkt-stats\"\n",
    "path4 = \"show hardware internal buffer info pkt-stats peak\"\n",
    "path5 = \"show system resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(rdd):\n",
    "    json_data = sqlContext.read.json(rdd)\n",
    "    # NX Data comes out weird, need to convert (Single Quotes instead of double quote issues)\n",
    "    data_broken = json_data.collect()[0].asDict()['_corrupt_record']\n",
    "    data = data_broken.replace(\"u'\", \"'\")\n",
    "    data = ast.literal_eval(data)\n",
    "    tags_master = {\n",
    "        'NodeID' : data['Telemetry']['node_id_str'],\n",
    "        'EncodingPath' : data['Telemetry']['encoding_path'].replace(\" \", \"-\")\n",
    "    }\n",
    "    for row in data[\"Rows\"]:\n",
    "        metrics = {\n",
    "            \"metric\": 'metric',\n",
    "            \"timestamp\": 'timestamp',\n",
    "            \"value\": 'value',\n",
    "            \"tags\": 'tags'}\n",
    "        content_keys_master = {}\n",
    "        content_keys = content_keys_master.copy()\n",
    "        tags = tags_master.copy()\n",
    "        # metrics['timestamp'] = data[\"Telemetry\"]['msg_timestamp']/1000\n",
    "        metrics['timestamp'] = time.time()\n",
    "        content = row['Content']\n",
    "        tsdb =  []\n",
    "        if data['Telemetry']['encoding_path'] == path1:\n",
    "            interface_loader(metrics, content, 'rx', tags)\n",
    "            interface_loader(metrics, content, 'tx', tags)\n",
    "        if data['Telemetry']['encoding_path'] == path2:\n",
    "            result = bgp_loader(metrics, content, tags)\n",
    "            tsdb_api_put(result)\n",
    "        if data['Telemetry']['encoding_path'] == path3:\n",
    "            result = buffers_peak_loader(metrics, content, tags)\n",
    "            tsdb_api_put(result)\n",
    "        if data['Telemetry']['encoding_path'] == path4:\n",
    "            result = buffers_peak_loader(metrics, content, tags)\n",
    "            tsdb_api_put(result)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buffers_loader(metrics, content, tags):\n",
    "    print(content)\n",
    "    tsdb = []\n",
    "    tags_copy = tags.copy()\n",
    "    instance = content['']['TABLE_module']['']['ROW_module']['']\n",
    "    tags_copy['module_number'] = instance['module_number']\n",
    "    values = instance['TABLE_instance']['']['ROW_instance']['']\n",
    "    tags_copy['instance'] = values['instance']\n",
    "    del values['instance']\n",
    "    for key in values.keys():\n",
    "        metrics_copy = metrics.copy()\n",
    "        metrics_copy['metric'] = key\n",
    "        metrics_copy['value'] = values[key]\n",
    "        metrics_copy['tags'] = tags_copy\n",
    "        tsdb.append(metrics_copy)\n",
    "    return json.loads(json.dumps(tsdb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buffers_peak_loader(metrics, content, tags):\n",
    "    tsdb = []\n",
    "    tags_copy = tags.copy()\n",
    "    instance = content['']['TABLE_module']['']['ROW_module']['']\n",
    "    tags_copy['module_number'] = instance['module_number']\n",
    "    values = instance['TABLE_instance']['']['ROW_instance']['']\n",
    "    tags_copy['instance'] = values['instance']\n",
    "    del values['instance']\n",
    "    for key in values.keys():\n",
    "        if key == 'TABLE_interface':\n",
    "            ports = values['TABLE_interface']['']['ROW_interface']['_PIPELINE_EDIT']\n",
    "            for item in ports:\n",
    "                if 'peak_stats_start' not in item and 'stats_start' not in item:\n",
    "                    tsdb_inner = []\n",
    "                    tags_inner = tags_copy.copy()\n",
    "                    tags_inner['front_port'] = item['front_port']\n",
    "                    del item['front_port']\n",
    "                    for counter in item.keys():\n",
    "                        metrics_copy = metrics.copy()\n",
    "                        metrics_copy['metric'] = counter\n",
    "                        metrics_copy['value'] = item[counter]\n",
    "                        metrics_copy['tags'] = tags_inner\n",
    "                        tsdb_inner.append(metrics_copy)\n",
    "                    tsdb_api_put(json.loads(json.dumps(tsdb_inner)))\n",
    "        else:\n",
    "            metrics_copy = metrics.copy()\n",
    "            metrics_copy['metric'] = key\n",
    "            metrics_copy['value'] = values[key]\n",
    "            metrics_copy['tags'] = tags_copy\n",
    "            tsdb.append(metrics_copy)\n",
    "    return json.loads(json.dumps(tsdb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bgp_loader(metrics, content, tags):\n",
    "    tsdb = []\n",
    "    tags_copy = tags.copy()\n",
    "    keys = ['totalpaths', 'totalnetworks']\n",
    "    vrf = content['']['TABLE_vrf']['']['ROW_vrf']['']\n",
    "    bgp = vrf['TABLE_af']['']['ROW_af']['_PIPELINE_EDIT']\n",
    "    for saf in bgp:\n",
    "        bgp_info = saf['TABLE_saf']['']['ROW_saf']['']\n",
    "        for key in keys:\n",
    "            if key in bgp_info:\n",
    "                metrics_copy = metrics.copy()\n",
    "                metrics_copy['metric'] = key\n",
    "                metrics_copy['value'] = bgp_info[key]\n",
    "                metrics_copy['tags'] = tags_copy\n",
    "                tsdb.append(metrics_copy)\n",
    "    return json.loads(json.dumps(tsdb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interface_loader(metrics, content, way, tags):\n",
    "    tsdb = []\n",
    "    tags_copy = tags.copy()\n",
    "    segment = content['']['TABLE_{}_counters'.format(way)]['']['ROW_{}_counters'.format(way)]['_PIPELINE_EDIT']\n",
    "    for interface in segment:\n",
    "        tags_copy['interface_name'] = interface['interface_{}'.format(way)]\n",
    "        for key in interface.keys():\n",
    "            if isinstance(interface[key], numbers.Number):\n",
    "                metrics_copy = metrics.copy()\n",
    "                metrics_copy['metric'] = key\n",
    "                metrics_copy['value'] = interface[key]\n",
    "                metrics_copy['tags'] = tags_copy\n",
    "                tsdb_api_put(metrics_copy)\n",
    "    return\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def system_loader(metrics, content, tags):\n",
    "    tsdb = []\n",
    "    tags_copy = tags.copy()\n",
    "    segment = content['']\n",
    "    metrics_wanted = [\n",
    "        'cpu_state_idle',\n",
    "        'cpu_state_kernel',\n",
    "        'cpu_state_user',\n",
    "        'memory_usage_used',\n",
    "        'memory_usage_free',\n",
    "        'memory_usage_total']\n",
    "    for key in metrics_wanted:\n",
    "        metrics_copy = metrics.copy()\n",
    "        metrics_copy['metric'] = key\n",
    "        metrics_copy['value'] = segment[key]\n",
    "        metrics_copy['tags'] = tags_copy\n",
    "        tsdb.append(metrics_copy)\n",
    "    return json.loads(tsdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsdb_api_put(data):\n",
    "    if data:\n",
    "        host = 'gspie-opentsdb.cisco.com:4242'\n",
    "        openTsdbUrl = 'http://' + host + '/api/put/details'\n",
    "        request = requests.post(openTsdbUrl, json = data)\n",
    "        if request.text:\n",
    "            print(request.text)\n",
    "            print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kafka_rdd.foreachRDD(lambda rdd: sc.parallelize(transform(rdd)))\n",
    "#kafka_rdd.pprint()\n",
    "ssc.start()\n",
    "#ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
